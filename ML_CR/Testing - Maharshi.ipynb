{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.28.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_size = 1000\n",
    "eviction = int(0.05 * cache_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files (Do not run unnesessarily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11687258"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = \"DATA/cheetah.cs.fiu.edu-110108-113008.3.blkparse\"\n",
    "\n",
    "df = pd.read_csv(train, sep=' ',header = None)\n",
    "df.columns = ['timestamp','pid','pname','blockNo', \\\n",
    "              'blockSize', 'readOrWrite', 'bdMajor', 'bdMinor', 'hash']\n",
    "\n",
    "trainBlockTrace = df['blockNo'].tolist()\n",
    "trainBlockTrace = trainBlockTrace[:int(len(trainBlockTrace)*0.5)]\n",
    "\n",
    "len(trainBlockTrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10344955"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"DATA/cheetah.cs.fiu.edu-110108-113008.4.blkparse\"\n",
    "\n",
    "df = pd.read_csv(test, sep=' ',header = None)\n",
    "df.columns = ['timestamp','pid','pname','blockNo', \\\n",
    "              'blockSize', 'readOrWrite', 'bdMajor', 'bdMinor', 'hash']\n",
    "\n",
    "testBlockTrace = df['blockNo'].tolist()\n",
    "testBlockTrace = testBlockTrace[:int(len(testBlockTrace)*0.5)]\n",
    "\n",
    "len(testBlockTrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFU(blocktrace, frame):\n",
    "    \n",
    "    cache = set()\n",
    "    cache_frequency = defaultdict(int)\n",
    "    frequency = defaultdict(int)\n",
    "    \n",
    "    hit, miss = 0, 0\n",
    "    \n",
    "    for block in tqdm(blocktrace, leave=False):\n",
    "        frequency[block] += 1\n",
    "        \n",
    "        if block in cache:\n",
    "            hit += 1\n",
    "            cache_frequency[block] += 1\n",
    "        \n",
    "        elif len(cache) < frame:\n",
    "            cache.add(block)\n",
    "            cache_frequency[block] += 1\n",
    "            miss += 1\n",
    "\n",
    "        else:\n",
    "            e, f = min(cache_frequency.items(), key=lambda a: a[1])\n",
    "            cache_frequency.pop(e)\n",
    "            cache.remove(e)\n",
    "            cache.add(block)\n",
    "            cache_frequency[block] = frequency[block]\n",
    "            miss += 1\n",
    "    \n",
    "    hitrate = hit / ( hit + miss )\n",
    "    return hitrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRU(blocktrace, frame):\n",
    "    \n",
    "    cache = set()\n",
    "    recency = deque()\n",
    "    hit, miss = 0, 0\n",
    "    \n",
    "    for block in tqdm(blocktrace, leave=False):\n",
    "        \n",
    "        if block in cache:\n",
    "            recency.remove(block)\n",
    "            recency.append(block)\n",
    "            hit += 1\n",
    "            \n",
    "        elif len(cache) < frame:\n",
    "            cache.add(block)\n",
    "            recency.append(block)\n",
    "            miss += 1\n",
    "            \n",
    "        else:\n",
    "            cache.remove(recency[0])\n",
    "            recency.popleft()\n",
    "            cache.add(block)\n",
    "            recency.append(block)\n",
    "            miss += 1\n",
    "    \n",
    "    hitrate = hit / (hit + miss)\n",
    "    return hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach (Maharshi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belady_opt_2(blocktrace, frame):\n",
    "    '''\n",
    "    INPUT\n",
    "    ============\n",
    "    blocktrace - list of blocks in sequence of request\n",
    "    cachesize - int value for capacity of the cache\n",
    "    \n",
    "    OUTPUT\n",
    "    ============\n",
    "    (1) hitrate (int)\n",
    "    (2) cache configuration and eviction block at time of miss (np.array)  \n",
    "    '''\n",
    "    \n",
    "    infinite_index = 100 * len(blocktrace) \n",
    "    # should be a large integer than block number\n",
    "    \n",
    "    block_index = defaultdict(deque) \n",
    "    # dictionary with block number as key and list\n",
    "    # of index value in blocktrace\n",
    "    \n",
    "    upcoming_index = defaultdict(int)\n",
    "    # dictionary with index number as key and value as block\n",
    "    \n",
    "    frequency = defaultdict(int)\n",
    "    # dictionary of block as key and number\n",
    "    # of times it's been requested so far\n",
    "    \n",
    "    recency = list()\n",
    "    # list of block in order of their request\n",
    "    \n",
    "    Cache = deque()\n",
    "    # Cache with block\n",
    "    \n",
    "    dataset = np.array([]).reshape(0,3*frame+1)\n",
    "    #columns represents the number of block in cache and \n",
    "    #3 is the number of features such as frequency, recency and block number\n",
    "    #+1 is for label 0-1\n",
    "    \n",
    "    hit, miss = 0, 0\n",
    "    \n",
    "    # populate the block_index\n",
    "    for i, block in enumerate(tqdm(blocktrace, \\\n",
    "                              desc=\"buidling index\", leave=False)):\n",
    "        block_index[block].append(i)\n",
    "        \n",
    "    # sequential block requests start\n",
    "    for i, block in enumerate(tqdm(blocktrace, desc=\"sequence\", leave=False)):\n",
    "        \n",
    "        # increament the frequency number for the block\n",
    "        frequency[block] += 1\n",
    "        \n",
    "        # make sure block has the value in block_index dictionary \n",
    "        # as current seq_number\n",
    "        if len(block_index[block]) != 0 and block_index[block][0] == i:\n",
    "            \n",
    "            # if yes, remove the first element of block_index[block]\n",
    "            block_index[block].popleft()\n",
    "        \n",
    "        # if block exist in current cache\n",
    "        if block in Cache:\n",
    "            \n",
    "            # increment hit\n",
    "            hit += 1\n",
    "            \n",
    "            # update the recency\n",
    "            recency.remove(block)\n",
    "            recency.append(block)\n",
    "            \n",
    "            # update upcoming_index\n",
    "            if i in upcoming_index:\n",
    "                \n",
    "                # delete old index\n",
    "                del upcoming_index[i]\n",
    "        \n",
    "                if len(block_index[block]) is not 0:\n",
    "                    # add new upcoming index\n",
    "                    upcoming_index[block_index[block][0]] = block\n",
    "                    # remove index from block_index\n",
    "                    block_index[block].popleft()\n",
    "                else:\n",
    "                    # add a large integer as index\n",
    "                    upcoming_index[infinite_index] = block\n",
    "                    # increament large integer\n",
    "                    infinite_index-=1\n",
    "           \n",
    "        # block not in current cache\n",
    "        else:\n",
    "            \n",
    "            # increament miss\n",
    "            miss += 1\n",
    "            \n",
    "            # if cache has no free space\n",
    "            if len(Cache) == frame:\n",
    "                  \n",
    "                # find the farthest i.e. max_index in upcoming_index\n",
    "                max_index = max(upcoming_index)\n",
    "\n",
    "                if (i % 1000 +1 == 1000):\n",
    "                    blockNo = np.array([i for i in Cache])\n",
    "                    blockNo = blockNo / np.linalg.norm(blockNo)\n",
    "                    recency_ = np.array([recency.index(i) for i in Cache])\n",
    "                    recency_ = recency_ / np.linalg.norm(recency_)\n",
    "                    frequency_ = np.array([frequency[i] for i in Cache])\n",
    "                    frequency_ = frequency_ / np.linalg.norm(frequency_)\n",
    "                    stack = np.column_stack((blockNo, recency_, frequency_)).reshape(1,frame*3)\n",
    "                    stack = np.append(stack, Cache.index(upcoming_index[max_index]))\n",
    "                    dataset = np.vstack((dataset, stack))\n",
    "                # remove the block with max_index from cache\n",
    "                Cache[Cache.index(upcoming_index[max_index])] = block\n",
    "\n",
    "                # remove the block with max_index from recency dict\n",
    "                recency.remove(upcoming_index[max_index])\n",
    "\n",
    "                # remove max_index element from upcoming_index\n",
    "                del upcoming_index[max_index]\n",
    "                    \n",
    "            \n",
    "            else:\n",
    "                 \n",
    "                # add block into Cache\n",
    "                Cache.append(block)\n",
    "\n",
    "            # add block into recency\n",
    "            recency.append(block)\n",
    "                \n",
    "            # add upcoming request of current block in upcoming_index\n",
    "            if len(block_index[block]) != 0:\n",
    "                \n",
    "                # add upcoming index of block\n",
    "                upcoming_index[block_index[block][0]] = block\n",
    "               \n",
    "                # remove the index from block_index \n",
    "                block_index[block].popleft()\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # add a large integer as index\n",
    "                upcoming_index[infinite_index] = block\n",
    "                \n",
    "                # increament high number\n",
    "                infinite_index -= 1\n",
    " \n",
    "            \n",
    "    # calculate hitrate\n",
    "    hitrate = hit / (hit + miss)\n",
    "\n",
    "    return hitrate, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainHitrate - create train data for training ML\n",
    "#### Make sure to clear global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='buidling index', max=11687258, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sequence', max=11687258, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "trainHitrate, trainData = belady_opt_2(trainBlockTrace, cache_size)\n",
    "X_train = trainData[:,:-1]\n",
    "Y_train = trainData[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = trainData[:,:-1]\n",
    "# Y = trainData[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestHitrate - create test data for testing ML\n",
    "#### Make sure to clear global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='buidling index', max=10344955, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='sequence', max=10344955, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "testHitrate, testData = belady_opt_2(testBlockTrace, cache_size)\n",
    "X_test = testData[:,:-1]\n",
    "Y_test = testData[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y.astype(int), test_size=0.3, \\\n",
    "#                                                     random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027165395465468603\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "print(logreg.score(X_test, Y_test))\n",
    "print(confusion_matrix(Y_test,logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019114790963916998\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 2]\n",
      " ...\n",
      " [0 1 5 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "# KNN.fit(X_train, Y_train)\n",
    "\n",
    "# print(KNN.score(X_test, Y_test))\n",
    "# print(confusion_matrix(Y_test,KNN.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008358583220144186\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "NN = MLPClassifier()\n",
    "NN.fit(X_train, Y_train)\n",
    "\n",
    "print(NN.score(X_test, Y_test))\n",
    "print(confusion_matrix(Y_test,NN.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitRate2(blocktrace, frame, model):\n",
    "    LFUDict = defaultdict(int)\n",
    "    LRUQ = []\n",
    "#     CacheTS = defaultdict(int)\n",
    "#     CachePID = defaultdict(int)\n",
    "\n",
    "    hit, miss = 0, 0\n",
    "\n",
    "    C = []\n",
    "    evictCacheIndex = np.array([])\n",
    "    #count=0\n",
    "    #seq_number = 0\n",
    "    for seq_number, block in enumerate(tqdm(blocktrace, desc=\"OPT\")):\n",
    "        #print(len(evictCacheIndex))\n",
    "        LFUDict[block] +=1\n",
    "        #CacheTS[blocktrace[seq_number]] = timestamp[seq_number]\n",
    "        #CachePID[blocktrace[seq_number]] = pid[seq_number]\n",
    "        if block in C:\n",
    "            hit+=1\n",
    "#             if C.index(block) in evictCacheIndex:\n",
    "#                 np.delete(evictCacheIndex, C.index(block))\n",
    "                \n",
    "            LRUQ.remove(block)\n",
    "            LRUQ.append(block)\n",
    "        else:\n",
    "            evictPos = -1\n",
    "            miss+=1\n",
    "            if len(C) == frame:\n",
    "                if len(evictCacheIndex) == 0: # call eviction candidates\n",
    "                    #X_test = getX(LRUQ, LFUDict, C)\n",
    "                    #X_test = getX(LRUQ, LFUDict, C, CacheTS, CachePID)\n",
    "                    blockNo = C / np.linalg.norm(C)\n",
    "                    recency_ = np.array([LRUQ.index(i) for i in C])\n",
    "                    recency_ = recency_ / np.linalg.norm(recency_)\n",
    "                    frequency_ = np.array([LFUDict[i] for i in C])\n",
    "                    frequency_ = frequency_ / np.linalg.norm(frequency_)\n",
    "                    stack = np.column_stack((blockNo, recency_, frequency_)).reshape(1,frame*3)\n",
    "                    #X_current = model.predict(stack)[0]\n",
    "                    Y_pred_prob = model.predict_proba(stack)\n",
    "                    evictCacheIndex = Y_pred_prob.argsort()[0][::-1][:eviction]\n",
    "                    # index of cache blocks that should be removed\n",
    "                    #return Y_pred_prob, evictCacheIndex\n",
    "                # evict from cache\n",
    "                evictPos = evictCacheIndex[0]\n",
    "                evictBlock = C[evictPos]\n",
    "                LRUQ.remove(evictBlock)\n",
    "                #del CacheTS [evictBlock]\n",
    "                #del CachePID [evictBlock]\n",
    "            if evictPos is -1:\n",
    "                C.append(block)\n",
    "            else:\n",
    "                C[evictPos] = block\n",
    "                evictCacheIndex = np.delete(evictCacheIndex, 0)\n",
    "            LRUQ.append(block)\n",
    "            #CacheTS [blocktrace[seq_number]] = timestamp[seq_number]\n",
    "            #CachePID [blocktrace[seq_number]] = pid[seq_number]\n",
    "        #seq_number += 1\n",
    "\n",
    "    hitrate = hit / (hit + miss)\n",
    "    print(hitrate)\n",
    "    return hitrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPT, LRU, LFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_size = 1000\n",
    "eviction = int(0.3 * cache_size)  #start from 50% size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testHitrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-05f374af03bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestHitrate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testHitrate' is not defined"
     ]
    }
   ],
   "source": [
    "testHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10344955), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05175343923680673"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRU(testBlockTrace, cache_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10344955), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05587989507929227"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFU(testBlockTrace, cache_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0357b5f912044ac08823ec6063f6d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='OPT', max=10344955, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.05210017829947061\n"
     ]
    }
   ],
   "source": [
    "LRhitrate = hitRate2(testBlockTrace, cache_size, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNhitrate = hitRate2(testBlockTrace, cache_size, KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NeuralNet Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f24875335a48e68a3092a9deefc625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='OPT', max=10344955, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.052337008715842646\n"
     ]
    }
   ],
   "source": [
    "NNhitrate = hitRate2(testBlockTrace, cache_size, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = load('logreg.joblib') \n",
    "NN = load('NN.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logreg.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(logreg, 'logreg.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(NN, 'NN.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
